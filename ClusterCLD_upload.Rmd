---
title: "Modeling social vulnerability to malaria in East Africa: A (spatially explicit) system-dynamics approach"  
author: "Linda Menk (Supervised by: Christian Neuwirth, Stefan Kienberger)"
date: "June 12, 2019"
output: 
  html_document:
    theme: yeti
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
      number_sections: true

---
<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
      text-align: justify;
      line-height: 1.5;
  }
th,td {  /* Table  */
  font-size: 10px;
  width:700px;
  white-space:nowrap;
}


h1.title {
  font-size: 38px;
  
}
h1 { /* Header 1 */
  font-size: 28px;
  
}
h2 { /* Header 2 */
    font-size: 22px;
  
}
h3 { /* Header 3 */
  font-size: 18px;
  f
  
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overall objective  
The overall objective of this project is to generate a Causal Loop Diagram which refelcts the relationships and feedback-loops between a set of indicators, which supposedly have an influence of a populations vulnerability towards malaria and it's side-effects.  
As the study region extends over five countries and is diverse in a multitude of aspects, the general assumption is, that **one Causal Loop Diagram can not sufficiently reflect the relationships and the systemic structure among the variables throughout the whole study region**. Therefore, the purpose of this work is to assess, whether the just stated assumption hold true by comparison of a CLD which treats the study region as one entity versus a series of CLDs, each representing meaningful subregions.  
To do this, a workflow was developed which divides the study region into subgroups, which are supposedly more capable of capturing the diverse relationships among the indicators throughout the study area. 
Finally, data-driven Causal Loop Diagrams (CLD) for each subgroup, as well as one for the whole study region, are generated.   
The name *Causal* Loop Diagram implies that the dispayed relationships are, in fact, of causal nature. However, distinguishing causality from mere correlation is something which can not automatically be derived from the data. Therefore, the data-driven CLDs are finally being modified in the attempt to only keep causal structures by using evidence from the literautre.
Further work will involve the transfer of the CLD's into executable system dynamics models (not part of this project).  


### Structure  
This section is describing the workflow which was developed in order to cluster the Geons, obtained by the HEALTHY FUTURES project, into groups of relatively homogenous correlation and to derive CDLs from these correlations. The analysis part consists of several steps, where each step is dependent on the outcome from the previous steps. Therefore, to maintain readability, each step is built up as follows:  

* Methodological background  
* Presentation of the results  
* Interpretation of the results and justification of the next step  

### Content  
The now following analysis is based on a script which was written in the programming language *R*, which is widely used for statistical computing. The script was utilized to:   

* test the data for normal/Gaussian distribution  
* assess the most appropriate number of clusters  
* compare how well differnt clustering algorithms perform on the data (k-means, AGglomerative (AGNES), DIvisive ANAlysis (DIANA)) in combination with different dissimilarity measures (Spearmans rho, Kendalls tau, Euclidean Distance) 
* utilize the best performing clustering algorithm for final cluster generation    
* gain insight into the clusters based on their descriptive statistics   
* visually analyze the clusters by mapping them  
* calculate Spearman's correlation coefficients between each variable pair  
* automatically generate Causal Loop Diagrams from significant correlations for each cluster  

  
```{r GEONS,message = FALSE}

#Loading necessary libraries
library(rgdal)
library(raster)
library(maptools)
library(devtools)
library(summarytools)
library(ggplot2)
library(sp)
library(factoextra)
library(fpc)
library(NbClust)
library(glue)

#Loading the spatial dataset (Shapefile) holding the data which is to be analyzed
GEONS_UTM<-readOGR("C:/Users/Linda/Desktop/Geoinformatik/Masterarbeit/03_Data/00_All/00_HealthyFutures/03_Final_Input/Final_Raster_10km_crop_undir_imp_outl/Final_Raster_10km_crop_undir_imp_outl/risk_malaria_vectri_2015base_UTM.shp")

#Making a dataframe from the spatial datset
GEONS_df<-data.frame( 
  "Immunity" = c (GEONS_UTM$IMMUN),
  "Distance to closest hospital" = c (GEONS_UTM$HOSP),
  "Distance to roads" = c (GEONS_UTM$ROADS),
  "Distance to closest urban center" = c (GEONS_UTM$URBAN),
  "Dependency ratio" = c (GEONS_UTM$DR),
  "Secondary/higher education" = c (GEONS_UTM$EDU),
  "HIV Prevalence" = c (GEONS_UTM$HIV_perc),
  "Child does not sleep under bednet" = c (GEONS_UTM$NOBEDNET),
  "People living on less than 2US$" = c (GEONS_UTM$POOR_nperc),
  "Stunting children" = c (GEONS_UTM$STUNT_perc),
  "Conflict density" = c (GEONS_UTM$CONFL),
  "WOCBA"=c(GEONS_UTM$WOCBA_nper),
  "Population change" = c (GEONS_UTM$Pop_press),
  "EIR"=c(GEONS_UTM$mean_eir_d)
  )

# Standardize the data in the GEONS_df dataframe
GEONS_df_scale<-data.frame(scale(GEONS_df))

```
# Workflow  
## Normal distribution  
### Method  

During the course of the now following statistical data analysis, the **correlations between the 14 varibales will be used as indicators to describe their relationships and interdependencies**. The utilization of correlations here is twofold: First, the correlations are used as dissimilarity measures to generate clusters of homogenous correlation, and later the correltions between the vulnerability indicators per cluster are used to generate the Causal Loop Diagrams.  
As there are different methods to calculate correlations, Pearsons Correlation Coefficient, Spearmans rho and Kendalls tau, being the most common, it is necessary to understand how they work and what requirements the data should meet, in order to yield meaningful results. The Pearson Correlation Coefficient is a measure of a linear relationship between two variables. Therefore, it should only be used if none of the assumptions underlying a general linear model are being violated by the data. One of these assumptions is that the data is normally distributed, as otherwise the tests of significance might not be reliable and outliers can easily distort the result. As **Spearmans Rho and Kendalls tau work in a different way** (being non-parametric estimators and measuring monotonicity relationships, therefore being more robust) **they can also be used on non-normally distributed data**. On a normally distributed dataset, the Pearson Correlation has the highest statistical efficacy while Spearmans rho and Kendalls tau score lower.  However, they are still adequately efficient and have the advantage of their robustness (Croux, Dehon: Influence functions of the Spearman and Kendall correlation measures), which makes them a good compromise. Kendalls tau has not been used as often as Spearmans rho in past studies, mostly because Kendalls tau is more difficult to compute. Computation intensity, however, is no longer an issue (Hauke, Kossowski: COMPARISON OF VALUES OF PEARSON’S AND SPEARMAN’S CORRELATION COEFFICIENTS ON THE SAME SETS OF DATA).  
Therefore, in the first section of this script, it is analyzed **whether the data is normally distributed or not**, in order to assess whether the Pearson Correlation can be utilized or if it is more advisable to use one of the non-parametric tests.  
The density plots are generated for visual inspection, accompanied by a Shapiro-Wilk test. The H0 hypotheses in the **Shapiro-Wilk test** is, that the data does not significantly differ from a normal distribution. If the resulting p-value is significant (<0.05), it can be assumed, that the data is not normally distributed.  

### Results  

```{r Normality test,message = FALSE,fig.height=20,fig.width=10}

# Testing if the data is normally distributed, by performing a Shapiro-Wilk normality test on every variable.
# The H0 hypothesis is, that the data does not significantly differ from a normal distribution. If the resulting p-value is significant (<0.05), the distribution is non-normal.
sh.Imm<-shapiro.test(GEONS_df$Immunity)
sh.Hos<-shapiro.test(GEONS_df$Distance.to.closest.hospital)
sh.Roa<-shapiro.test(GEONS_df$Distance.to.roads)
sh.Urb<-shapiro.test(GEONS_df$Distance.to.closest.urban.center)
sh.DR<-shapiro.test(GEONS_df$Dependency.ratio)
sh.Edu<-shapiro.test(GEONS_df$Secondary.higher.education)
sh.HIV<-shapiro.test(GEONS_df$HIV.Prevalence)
sh.Bed<-shapiro.test(GEONS_df$Child.does.not.sleep.under.bednet)
sh.Poor<-shapiro.test(GEONS_df$People.living.on.less.than.2US.)
sh.Stu<-shapiro.test(GEONS_df$Stunting.children)
sh.Con<-shapiro.test(GEONS_df$Conflict.density)
sh.WOC<-shapiro.test(GEONS_df$WOCBA)
sh.Pop<-shapiro.test(GEONS_df$Population.change)
sh.EIR<-shapiro.test(GEONS_df$EIR)

#Creating plots to diaplay the results from the Shapiro-Wilk test and density plots
library("ggpubr")
dImm<-ggdensity(GEONS_df$Immunity, caption=glue("Shapiro-Wilk test p-value: {sh.Imm$p.value}"),
          main = "Immunity to malaria", font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          xlab = "Immunity (%)")

dHos<-ggdensity(GEONS_df$Distance.to.closest.hospital, caption=glue("Shapiro-Wilk test p-value: {sh.Hos$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Distance to hospital",
          xlab = "Distance to nearest hospital (km)")

dRoa<-ggdensity(GEONS_df$Distance.to.roads, caption=glue("Shapiro-Wilk test p-value: {sh.Roa$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Distance to roads/transportation networks",
          xlab = "Path distance (least cost)")

dUrb<-ggdensity(GEONS_df$Distance.to.closest.urban.center, caption=glue("Shapiro-Wilk test p-value: {sh.Urb$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Travel time to clostest urban centers/markets",
          xlab = "Travel time (hours)")

dDR<-ggdensity(GEONS_df$Dependency.ratio, caption=glue("Shapiro-Wilk test p-value: {sh.DR$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Dependency ratio",
          xlab = "Dependency ratio (%)")

dEdu<-ggdensity(GEONS_df$Secondary.higher.education, caption=glue("Shapiro-Wilk test p-value: {sh.Edu$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "People with secondary of higher education",
          xlab = "Secondary or higher education (%)")

dHIV<-ggdensity(GEONS_df$HIV.Prevalence, caption=glue("Shapiro-Wilk test p-value: {sh.HIV$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "HIV prevalence",
          xlab = "HIV prevalence among 13-49 year olds (%)")

dBed<-ggdensity(GEONS_df$Child.does.not.sleep.under.bednet, caption=glue("Shapiro-Wilk test p-value: {sh.Bed$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Bednet usage",
          xlab = "Child did not sleep under net last night (%)")

dPoor<-ggdensity(GEONS_df$People.living.on.less.than.2US., caption=glue("Shapiro-Wilk test p-value: {sh.Poor$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "People living on less than 2US$ per day",
          xlab = "People living on less than 2US$ per day (%)")

dStu<-ggdensity(GEONS_df$Stunting.children, caption=glue("Shapiro-Wilk test p-value: {sh.Stu$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Stunting children under 5 years",
          xlab = "Stunting children under 5 years (%)")

dCon<-ggdensity(GEONS_df$Conflict.density, caption=glue("Shapiro-Wilk test p-value: {sh.Con$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Armed conflict events",
          xlab = "Conflict density (km²)")

dWOC<-ggdensity(GEONS_df$WOCBA, caption=glue("Shapiro-Wilk test p-value: {sh.WOC$p.value}"),font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Women of childbearing age (13-49 years)",
          xlab = "WOCBA (%)")

dPop<-ggdensity(GEONS_df$Population.change,caption=glue("Shapiro-Wilk test p-value: {sh.Pop$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Population change",
          xlab = "Population change (1970-2013)")

dEIR<-ggdensity(GEONS_df$EIR,caption=glue("Shapiro-Wilk test p-value: {sh.EIR$p.value}"), font.main=15, font.x=13, font.y=13, font.tickslab=13, font.caption=13,
          main = "Entomological Inoculation Rate",
          xlab = "EIR (%)")

#Arranging the plots in a grid
gridExtra::grid.arrange(dImm, dHos, dRoa, dUrb,dDR, dEdu, dHIV, dBed,dPoor, dStu, dCon, dWOC,dPop, dEIR,ncol=2)  

```

### Interpretation  
The resuts show, that the Shapiro-Wilk test was significant (being far below the 5% threshold) in every case. In addition, the density plots show that the data is higly skewed in most cases. This implies, that **none of the data is normally distributed**, and it is not advisable to use the Pearsons method for the analysis of correlation, but rather use the Spearmans Rho or Kendalls tau. Croux and Dehound (2010) found that the Kendalls tau is slightly more efficient and robust than the Spearmans rho. However, as both methods will in most cases lead to very similar conclusions (Hauke, Kossowski,2011), before deciding for one or the other, the next section will investiage which method fits the data used in this project better.


## Cluster Analysis   

Jain (1988) described the characteristics of a good cluster as follows: 
  (1) Instances, in the same cluster, must be similar as much as possible;
  (2) Instances, in the different clusters, must be different as much as possible;
  (3) Measurement for similarity and dissimilarity must be clear and have the practical meaning;  
@article{jain1988algorithms,
  title={Algorithms for clustering data},
  author={Jain, Anil K and Dubes, Richard C},
  year={1988},
  publisher={Prentice-Hall, Inc.}
} 
Xu (2005) described four consecutive steps a typical cluster analysis consists of:  
  (1) Feature selection or extraction  
  (2) Clustering algorithm design or selection  
  (3) Cluster validation  
  (4) Results interpretation  
@article{xu2005survey,
  title={Survey of clustering algorithms},
  author={Xu, Rui and Wunsch, Donald C},
  year={2005},
  publisher={Institute of Electrical and Electronics Engineers (IEEE)}
}  

The next chapter descirbes the steps 2 and 3, the selection and parametrization of clustering algorithms and the validation of the resulting clusters in terms of inner homogeneity. Chaper no. ??? - ??? deals with the interpretation of the results. Step number 1 is omitted in this particular case, as the systemic nature of all the selected vulnerability indicators are of interest. 


### Method  
Objective of the next step is to generate meaningful clusters from the data. The clustering process can be carried out with a number of different methods, where each requires the analyst to make decisions concering their parametrization. The next part offers methods to validate which method and parametrization fits the data best.  

### Clustering algorithms  
The function *eclust* from the R *factoextra* package was used to derive different types of clusters. The function allows the user to choose a clustering algorithm and to specify the number of clusters and the metrics which should be used for the calculation of dissimilarities between variables and between clusters. In this project, three different clustering algorithms were compared, namely *AGNES*, *DIANA* and *K-Means*. These three were chosen as K-Means is a wiedely used algorithm with many successfull appliactions, while *AGNES* and *DIANA* yielded the most promising results. However, a wide range of other clustering algorithms exisits. For further reading, Xu (2015) gives and overview over the 26 most commonly used ones.  
@article{xu2015comprehensive,
  title={A comprehensive survey of clustering algorithms},
  author={Xu, Dongkuan and Tian, Yingjie},
  journal={Annals of Data Science},
  volume={2},
  number={2},
  pages={165--193},
  year={2015},
  publisher={Springer}
}
**AGNES and DIANA are hierarchical clustering algorithms, in contrast to K-Means**, which is a method of vector quantization which is based on partitioning. 
Concerning the hierarchical methods AGNES and DIANA - **AGNES** is an agglomertative - (AGNES stands for **AGglomerative NESting**), while **DIANA** (**DIvisive ANAlysis**) is a divisive clustering algorithm. Agglomerative means that initially each datapoint starts off as it's own cluster and is stepwise megerged with its closest neighbour until a maximum distance or a specified number of clusters is reached. The divisive method starts with the opposite assumption that all of the observations belong to a single cluster. Consecutively, the cluster is divided into the two clusters with maximum heterogeneity between them - and so on, until the specified number of clusters is reached.  
K-means generates clusters by randomly placing a number of *k* centroids or seeds somewhere within the data. *K* is the number of desired clusters which was in advance specified by the analyst. Every datapoint is then assigned to a cluster, based on which centroid it is closest to (what *closeness* means is specified by the chosen distance measure). In the next step, the algorithm shifts the location of the centriod to the center of the just generated clusters. This may cause datapoints to now be closer to a centroid of a cluster which they were not originally assigned to. These points are then removed from their original cluster and are assigned to the cluster which now fits better. These two steps of shifting the centriods and reassigning datapoints is repeated until the within cluster variation cannot be reduced any further. 

An advantage the hierarchical clustering methods have over k-means is that they always produce the same result (reproducibility). The k-means algorithm needs to randomly choose datapoints which functions as the "seed", from which the consecutive clustering process starts. Yet, which datapoints are chosen as the seed can heavily influence the composition of the resulting clusters. As the seed is by default planted randomly, every run can produce a different output. This can be avioded by specifying the seed manually, however, in this case the analyst has to justify the decision of chosing a specific seed over another.  

#### Parametrization  
All the clustering algorithms need to be parametrizised in terms of *What measure should define the distance between datapoints?*, *What measure should define the distance between clusters?* and *How many clusters are to be generated?*. The next section describes some of the available options.  

#### Distance measure between datapoints  
The most common way of defining distance is to use the euclidean distance, a straight-line distance between two points in Euclidean space. In some cases, e.g. in crime data analysis, the Manhattan distance (also called taxicab or cityblock distance) is used, where the distance between two points is measured along axes at right angles.  
Another measure, which is widely used in gene expression data analysis in the field of Bioinformatics, is the so-called *correlation-based distance*. This measure of distance is of great interest in this project, as the overall objective is to define regions with similar underlying systemic structures. Therefore, spatial entities which share similar correlations between their attributes should be grouped together, rather than regions which only share similar data values. Using correlation as dissimilarity measure is possible in combiantion with the hierarchical clustering algorithms, but not in combination with the k-means algorithm. K-means cannot take correlations as distance measures, as it can minimize the squared Euclidean Distances between two datapoints, but no other arbitrary measures of distance. 

Datawise it is arbitrary which distance measure is used - so it is on the analyst to assess what distance measure yields the most meaningful resut for the dataset of concern. Due to the purpose of the project, a clustering algorithm which is capable of measuring dissimilarity in the form of correlations is clearly given priority over one which is not. However, the K-Means algorithm is still incorporated in the analysis to assess whether the resulting clusters significantly differ from those using correlation.    

#### Distance measure between clusters  
For all the methods, the distace between clusters is calculated by using Ward's Method. 
"This method works out which observations to group based on reducing the sum of squared distances of each observation from the average observation in a cluster. This is often appropriate as this concept of distance matches the standard assumptions of how to compute differences between groups in statistics (e.g., ANOVA, MANOVA)." (https://www.displayr.com/what-is-hierarchical-clustering/)

#### Number of clusters  
A parameter which all of the beforementioned clustering techniques require is the **desired number of clusters**. Generating a so-called **Scree Plot** can support the analyst in chosing the most suitable number of clusters, as it shows how the **within-cluster sum of squares decreases with each added cluster**. The lower the total sum of squares within each cluster, the more homogenuous they are. 
Therefore, the first step of the following cluster analysis is to estimate the most purposeful number of clusters for the k-means algorithm, as well as for the hierarchical algorithm AGNES. Generating a Scree Plot for DIANA is at this time not available in the *eclust* function. However, it can be assumed, that the result for DIANA would look similar to AGNES.  
  
 

Initially, the **data is standardized** as the values of the dissimilarity measure are closely related to the scale on which the measurements are made. The standardization prevents variables with particularly high values from dominating the clustering processto and values measured on different scales are made comparable. After the standardization, the average of each variable is 0 and the standard deviation is approximately 1.  

#### Operating principle: Correlation based distance  
The objective of this work is to derive clusters of homogenous systemic sturctures. Therefore, it is necessary to use the correlations between the geons as the basis for clustering, rather than the mere distance between datavalues. The function *eclust* offers the parameter *hc_metric*, which is capable of taking correlations as input. To achieve this, the function calculates the correlations of every variable for every geon and writes the resulting values into a table, in this case with a length of 11694 entries (n=108; 108*107/2=5778). This vector then functions as the input for the clustering algorithm.   



  

#### Result: Number of clusters  
```{r Scree Plot, message = FALSE}

#Creating Scree Plots (or "Elbow" plots) to see the decrease of total sum of squares with each added cluster

#Creating a Scree plot object
nch<-fviz_nbclust(GEONS_df_scale, hcut, method = "wss") +
#Defining where to draw the dashed line
geom_vline(xintercept = 3, linetype = 2)

#Creating a Scree plot object
nckm<-fviz_nbclust(GEONS_df_scale, kmeans, method = "wss") +
#Defining where to draw the dashed line
geom_vline(xintercept=4,linetype = 2)

#Plotting the Scree plots 
pnch<-ggpubr::ggpar(nch,submain="(for AGNES clustering)")
pnckm<-ggpubr::ggpar(nckm,submain="(for k-means clustering)")

#Arranging the plots in a grid
gridExtra::grid.arrange(pnch, pnckm ,ncol=2)  

```

The Scree Plots show, that for AGNES, either three or four cluster would result in the greatest impact on reducing the within sum of squares. For the k-means, it is more obvious that the curve flattens after four clusters. 

Therefore, in the further analysis, for the AGNES algorithm three clusters will be chosen as the number of clusters to analyze, with the option to extend to four. For the k-means, four clusters seem to yield the best results.  

### Method: Data Clusters  
Now that the number of clusters is specified, the decisions concerning which clustering algorithm in combination with which distance metric to choose, is still to be made. Both, Spearmans rho and Kendalls tau could be used as correlation based distance measures. Therefore, each of them is tested together with AGNES and DIANA clustering respectively, as well as the clusters generated by K-Means.

```{r Clustering, message = FALSE,fig.height=15,fig.width=12}

#The following code produces Scatter Plots for the different methods and combinations tried

#K-means with Euclidean Distance
kmean <- eclust(GEONS_df_scale, "kmeans", k = 4, nstart = 42, graph = FALSE,hc_metric="euclidean")
distKmean<-fviz_cluster(kmean, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution k-means",font.main=15)

#AGNES with Spearman
CorA <- eclust(GEONS_df_scale, "agnes", k = 3, graph = FALSE,hc_metric="spearman")
distCorA<-fviz_cluster(CorA, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution AGNES with Spearmans rho",font.main=15)

#AGNES with Kendall
CorAK <- eclust(GEONS_df_scale, "agnes", k = 3, graph = FALSE,hc_metric="kendall")
distCorAK<-fviz_cluster(CorAK, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution AGNES with Kendalls tau",font.main=15)                 

#DIANA with Spearman 3 clusters
CorD <- eclust(GEONS_df_scale, "diana", k = 3, graph = FALSE,hc_metric="spearman")
distCorD<-fviz_cluster(CorD, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution DIANA with Spearmans rho (3 clusters)",font.main=15)

#DIANA with Spearman 4 clusters
CorD4 <- eclust(GEONS_df_scale, "diana", k = 4, graph = FALSE,hc_metric="spearman")
distCorD4<-fviz_cluster(CorD4, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution DIANA with Spearmans rho (4 clusters)",font.main=15)

#DIANA with Kendall
CorDK <- eclust(GEONS_df_scale, "diana", k = 3, graph = FALSE,hc_metric="kendall")
distCorDK<-fviz_cluster(CorDK, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution DIANA with Kendalls tau",font.main=15)

#Arranging the plots
gridExtra::grid.arrange(distKmean, distCorA, distCorAK,distCorD,distCorD4,distCorDK, nrow = 3)

```

### Result: Data Clusters  
The following plots visualize the different clustering results. *AGNES with Spearmans rho*, *AGNES with Kendalls tau*, *DIANA with Spearmans rho (3 clusters)* and *DIANA with Kendalls tau* all show a rather similar distribution with only minor differences. The cluster for *K-Menans* and *DIANA with Spearmans rho (4clusters) naturally differ from the other four in their appearance, as they incorporate four clusters. Yet, compared to each other, they also show a rather differing pattern.  
### Method: Silhouette Plots  
The Silhouette plots help to visually validate cluster quality. **The higher the cluster score of an object is, the better it fits into the cluster it was assigned to and the more dissimilar it is to the other clusters**. This means, the higher the average cluster width and the corresponding average silhouette widths, the more homogenous a cluster is and the more dissimilar it is to the other clusters. The next section displays the silhouette plots for each clustering method which was tested in the last section.  

```{r Silhouette Plots,  message=FALSE,fig.height=15,fig.width=15}

#Creating a Silhouette-Obeject for each tried combination
DIANA<-fviz_silhouette(CorD,print.summary = 	FALSE)
AGNES3<-fviz_silhouette(CorA,print.summary = 	FALSE)
DIANA4<-fviz_silhouette(CorD4,print.summary = 	FALSE)
DK<-fviz_silhouette(CorDK,print.summary = 	FALSE)
KMEAN<-fviz_silhouette(kmean,print.summary = 	FALSE)
AGNESK<-fviz_silhouette(CorAK,print.summary = 	FALSE)

#Extracting information of average silhouette width to be displayed in the final Silhouette plots
submainp1<- format(CorD$silinfo$avg.width,digits=3)
submainp2<- format(CorA$silinfo$avg.width,digits=3)
submainp3<- format(CorDK$silinfo$avg.width,digits=3)
submainp4<- format(kmean$silinfo$avg.width,digits=3)
submainp5<- format(CorAK$silinfo$avg.width,digits=3)
submainp6<- format(CorD4$silinfo$avg.width,digits=3)


#Extracting information of average cluster width to be displayed in the final Silhouette plots
CorD1<-format(CorD$silinfo$clus.avg.widths[1],digits=3)
CorD2<-format(CorD$silinfo$clus.avg.widths[2],digits=3)
CorD3<-format(CorD$silinfo$clus.avg.widths[3],digits=3)

CorA1<-format(CorA$silinfo$clus.avg.widths[1],digits=3)
CorA2<-format(CorA$silinfo$clus.avg.widths[2],digits=3)
CorA3<-format(CorA$silinfo$clus.avg.widths[3],digits=3)

CorD41<-format(CorD4$silinfo$clus.avg.widths[1],digits=3)
CorD42<-format(CorD4$silinfo$clus.avg.widths[2],digits=3)
CorD43<-format(CorD4$silinfo$clus.avg.widths[3],digits=3)
CorD44<-format(CorD4$silinfo$clus.avg.widths[4],digits=3)

CorDK1<-format(CorDK$silinfo$clus.avg.widths[1],digits=3)
CorDK2<-format(CorDK$silinfo$clus.avg.widths[2],digits=3)
CorDK3<-format(CorDK$silinfo$clus.avg.widths[3],digits=3)

CorAK1<-format(CorAK$silinfo$clus.avg.widths[1],digits=3)
CorAK2<-format(CorAK$silinfo$clus.avg.widths[2],digits=3)
CorAK3<-format(CorAK$silinfo$clus.avg.widths[3],digits=3)

km1<-format(kmean$silinfo$clus.avg.widths[1],digits=3)
km2<-format(kmean$silinfo$clus.avg.widths[2],digits=3)
km3<-format(kmean$silinfo$clus.avg.widths[3],digits=3)
km4<-format(kmean$silinfo$clus.avg.widths[4],digits=3)

#Creating and defining the visual characteristics of the Silhouette plots
p1<-ggpubr::ggpar(DIANA,main="DIANA with Spearman Rho",submain=glue("Average silhouette width: {submainp1}"),caption=glue("Average cluster width 1: {CorD1}   2: {CorD2}  3: {CorD3}"),xlab=FALSE,legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p2<-ggpubr::ggpar(AGNES3,main="AGNES with Spearman Rho (3 cluster)",submain=glue("Average silhouette width: {submainp2}"),caption=glue("Average cluster width 1: {CorA1}   2: {CorA2}  3: {CorA3}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p3<-ggpubr::ggpar(DK,main="DIANA with Kendalls tau",submain=glue("Average silhouette width: {submainp3}"),caption=glue("Average cluster width 1: {CorDK1}   2: {CorDK2}  3: {CorDK3}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p4<-ggpubr::ggpar(KMEAN,main="KMEAN with Euclidean Distance",submain=glue("Average silhouette width: {submainp4}"),caption=glue("Average cluster width 1: {km1}   2: {km2}  3: {km3}  3: {km4}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p5<-ggpubr::ggpar(AGNESK,main="AGNES with Kendalls tau",submain=glue("Average silhouette width: {submainp5}"),caption=glue("Average cluster width 1: {CorAK1}   2: {CorAK2}  3: {CorAK3}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p6<-ggpubr::ggpar(DIANA4,main="DIANA with Spearman Rho (4 cluster)",submain=glue("Average silhouette width: {submainp6}"),caption=glue("Average cluster width 1: {CorD41}   2: {CorD42}  3: {CorD43}  4: {CorD44}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

#Arraging the plots in a grid
gridExtra::grid.arrange(p1, p2,p5,p3,p4,p6, nrow = 3,ncol=2)

```

The Silhouette plots reveal, that the **divisive method in combination with Spearmans rho as distance measure performs best** (has the highest overall average Silhouette width), **followed by** the **agglomerative method with Spearmans rho**, which even outperforms the first method in two of the three average cluster widths. Using Kendalls tau as distance measures or k-means with euclidean distance are scoring noticeably lower.  
As AGNES in combination with Spearmans rho give the overall highest average cluster width for Cluster No. 1 and 2, whilst scoring low at Cluster No. 3 it seems appropriate to investiagte whether four clusters would yield an even better result.  

#### A word on K-Means   
Independent from the fact that the K-Means algorithm has yielded the lowest average silhouette width, it would not have made a proper basis for the further analysis anyway: The aspired Causal Loop Diagrams are based on the correlations between the underlying Geons. Therefore, using the correlations as distance metric during the clustering process is more purposeful in order to generate meaningful clusters than using the mere distance between datpoints as the basis. The option to use correlation based distance metrics is not available in the used *eclust* function, hence, K-Means can not be purposefully utilized in the further course of the project.  




```{r Regionalizing,message = FALSE,fig.height=5,fig.width=15}

# Save the cluster number of each geon in the GEONS_df dataframe as column 'DIANA'
GEONS_UTM$DIANA <- as.factor(CorD$cluster)

#Split the dataframe GEONS_df into three groups based on the number in the cloumn 'DIANA'
classD <- split(GEONS_df, GEONS_UTM$DIANA)

#Writing the three fragments of the former GEONS_df dataframe to new dataframes. Each dataframe now represents one cluster. 
DRegion1<-data.frame(classD$`1`)
DRegion2<-data.frame(classD$`2`)
DRegion3<-data.frame(classD$`3`)

```

### Interpretation   

This approach managed to increase the average cluster width of Cluster No. 3 from 0.138 to 0.318 and 0.351 respectively. 
The average silhouette width is calculated as follows:  
$$X = /frac{n_1 /cdot c_1 + n_2 /cdot c_2 ... + n_n /cdot c_n}{N}$$  
where $n$ is the number of objects per cluster, $c$ is the average cluster width and $N$ is the total amount of objects. When combining Cluster No. 1 and 2 from AGNES and the re-clustered Cluster No. 3, this gives:  
$$0,389 = /frac{44 /cdot 0.349 + 20 /cdot 0.606 + 26 /cdot 0.318 + 18 /cdot 0.351}{108}$$  

In this case, manual intervention into the clustering process managed to increase the average cluster width and consequently the average silhouette width from 0.311 (3 Clusters) respecively 0.325 (4 Clusters) to 0.389. The difference of 6.4 ponits to the automatically generated four clusters is clearly an improvement. The drawback herein is, that the transferability of the workflow is reduced and one may argue that this intervention is overfitting the model. However, as the objective of this data analysis is to gain as much insight as possible into the systemic structures of the study region, the author sees this operation as justifiable and an improvement of the clustering process.  


### Are the correlation based distance measure and the euclidean distance producing differing outputs?  
Another question which should be investigated at this point is if the correlation-based distance measure does produce different clusters as if the euclidean distance was used?  
The original intention was to compare the best scoring clusters derived by DIANA in combination with Spearmans rho to DIANA in combination with Euclidean Distance. However, the latter combination resuluts in the algorithm assigning all Geons to the same cluster, except for two, which are the sole members of their own clusters and this presumably only happens because the algorithm is instructed to generate exactly three clusters.  
Therefore, the comparison is now carried out based on AGNES in combination with Spearmans rho and Euclidean Distance.  
```{r Comparison,message=FALSE,fig.width=7,fig.height=10}

#Comparing whether AGNES in combination with Spearman and Euclidean Distance as distance measure produces different pattern, datawise and spatially

#Clustering AGNES in combination with Spearman
CorA <- eclust(GEONS_df_scale, "agnes", k = 3, graph = FALSE,hc_metric="spearman")
# Visualize clusters as plots
distCorA<-fviz_cluster(CorA, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution AGNES with Spearmans rho",font.main=15)

#Clustering AGNES in combination with Euclidean Distance
CorAe <- eclust(GEONS_df_scale, "agnes", k = 3, graph = FALSE,hc_metric="euclidean")
# Visualize clusters as plots
distCorAe<-fviz_cluster(CorAe, geom = "point", ellipse.type = "norm",
             palette = "jco", ggtheme = theme_minimal(),main="Distribution AGNES with Euclidean Distance",font.main=15)

#Arranging plots in a grid
gridExtra::grid.arrange(distCorA, distCorAe,nrow=2)

#Creating Silhouette objects from the clusters
AGNES<-fviz_silhouette(CorA,print.summary = 	FALSE)
AGNESe<-fviz_silhouette(CorAe,print.summary = 	FALSE)

#Extracting information for average silhouette width to be displayed in the plot
submainA<- format(CorA$silinfo$avg.width,digits=3)
submainAe<- format(CorAe$silinfo$avg.width,digits=3)

#Extracting information for average cluster width to be displayed in the plot
CorA1<-format(CorA$silinfo$clus.avg.widths[1],digits=3)
CorA2<-format(CorA$silinfo$clus.avg.widths[2],digits=3)
CorA3<-format(CorA$silinfo$clus.avg.widths[3],digits=3)

CorAe1<-format(CorAe$silinfo$clus.avg.widths[1],digits=3)
CorAe2<-format(CorAe$silinfo$clus.avg.widths[2],digits=3)
CorAe3<-format(CorAe$silinfo$clus.avg.widths[3],digits=3)

#Creating and defining visual characteristics of the Silhouette plots
p1<-ggpubr::ggpar(AGNES,main="AGNES with Spearman Rho",submain=glue("Average silhouette width: {submainA}"),caption=glue("Average cluster width 1: {CorA1}   2: {CorA2}  3: {CorA3}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

p2<-ggpubr::ggpar(AGNESe,main="AGNES with Euclidean Distance",submain=glue("Average silhouette width: {submainAe}"),caption=glue("Average cluster width 1: {CorAe1}   2: {CorAe2}  3: {CorAe3}"),legend.title = "Cluster No.",font.main=20, font.x=15, font.y=15,font.subtitle=15,font.legend=15,font.caption=15,tickslab=FALSE)

#Arranging plots in a grid
gridExtra::grid.arrange(p1, p2,nrow=2)  

#Writing  the results in two columns called AGNES and AGNESe into the spatial dataset GEONS_UTM in order to make them displayable on a map
GEONS_UTM$AGNES <- as.factor(CorA$cluster)
GEONS_UTM$AGNESe <- as.factor(CorAe$cluster)

#Loading mapping library
library(tmap)

#Plot two maps, one displaying results yielded from a combination of the clustering algorithm AGNES in combination with Spearman correlation as distance measure and the other with Euclidean Distance 
tmap_mode("view")
AGNES.Spearman<-tm_shape(GEONS_UTM)+tm_polygons("AGNES",palette="-Spectral", style="fixed", title="Cluster ID",id="DIANA",alpha=0.8)+tm_layout(title="Study Region Clustered by AGNES using Spearmans Rho as distance measure with populated places") +
tm_legend(show = TRUE)

tmap_mode("view")
AGNES.Euclidean<-tm_shape(GEONS_UTM)+tm_polygons("AGNESe",palette="-Spectral", style="fixed", title="Cluster ID",id="DIANA",alpha=0.8)+tm_layout(title="Study Region Clustered by AGNES using Euclidean Distance ") +
tm_legend(show = TRUE)

tmap_arrange(AGNES.Spearman,AGNES.Euclidean,ncol=2,sync=TRUE)
#Write the results to a Shapefile in order to map them with another mapping software
#writeOGR(GEONS_UTM, ".", "AGNESe", driver="ESRI Shapefile")
#writeOGR(GEONS_UTM, ".", "AGNES", driver="ESRI Shapefile")

```

### Result  
The tests show, that the results in fact do differ in (a) how the data is custered, (b) the clusters homogeneity and (c) their spatial distribution. This implies that the choice of dissimilarity measure produce inherently different results and justifies the approach to utilize correlations as dissimilarity measure in order to yield clusters of similar underlying systemic structures.  

### Maping the clusters  
When the clusters are visualized on a map something interesting can be observed: Even though spatial connectivity or proximity has not been a factor in the clustering process, the clusters are almost completely spatially adjacent. Waldo Toblers *First Law of Geography* states that *everything is related to everything else, but near things are more related than distant things* (Tobler, 1970). From this analysis it could be assumed, that near things are also more *correlated* than distant things. Further work could investiagte if this assumption still holds true in other contexts.  

```{r Region mapping,message = FALSE}

#Mapping the final cluster to see what spatial patterns did emerge

#For orientation, a Shapefile contatining populated places retrieved from https://www.naturalearthdata.com is loaded into the map
populatedplaces<-readOGR("C:/Users/Linda/Desktop/Geoinformatik/Masterarbeit/03_Data/00_All/03_Capitals/ne_10m_populated_places_simple_clip.shp")
#The size of the points representing a populated place is determined by their number of inhabitants
populatedplaces$rank_max<-as.numeric(populatedplaces$rank_max)

#Plot the map
tmap_mode("view")
tm_shape(GEONS_UTM)+tm_polygons("DIANA",palette="-Spectral", style="fixed", title="Cluster ID",id="DIANA",alpha=0.8)+tm_layout(title="Study Region Clustered by DIANA using Spearmans Rho as distance measure with populated places")+tm_shape(populatedplaces) +tm_symbols(col = "yellow",alpha=0.5, size = "rank_max", scale = 0.8,id="pop_max") +
tm_legend(show = TRUE)






#writeOGR(GEONS_UTM, ".", "DIANA", driver="ESRI Shapefile")
```


```{r shp K means export}
#Exporting data
#K_means<-GEONS_UTM
#writeOGR(K_means, dsn = "C:/Temp" , layer = "Class_Kmeans_res",driver="ESRI Shapefile")
```

### Summary Statistics  
The following tables show the descriptive statistics of the four derived clusters. For the clustering, the data was used in its standardized form - for easier interpretation, the original values are shown here.  

**Cluster 1** is characterized by **high acceccibility** (*Distance to Roads*, - *Urban Centers*, - *Closest Hospital*) and **high population density** (265 inhabitants per km² on average) which makes it the most urban/urbanized region. Consequently, it differs from the other regions by it's high *Conflict Density*, while on the other hand, the percentage of persons with *Secondary or Higher Education* is the highest by a clear margin.The share of *People living on less than 2US$ per day* is slightly higher than in region 2, while still being on the lower end of the spectrum. The *EIR* in this cluster is the lowest, while the *Immunity* still relatively high. However, the use of bednets is not widely spread. The *Dependency Ratio* ranges second highest, accompanied by the highest values of *HIV prevalence* and *Number of stunting children*.  

**Cluster 2** has the lowest population density and holds relatiively poor accessibility values. While Cluster 2 and 4 can both be described as rural, Cluster 2 is defined by a relatively high *Conflict density* as well as a relatively high *EIR* paired with alomst no *Immunity* and a shortcoming of *Use of bednets*. Slighly less people with *Secondary or higher education* than in Cluster 3 and 4 are living in Cluster 2, while on the other hand, the share of *People living on less than 2 US$ per day* and *HIV Prevalence* are the lowest of all Clusters. Also, the share of *Women of childbearing age* is the highest, while the *Dependency ration* and the *Number of stunting children* are comparably low.  

**Cluster 3** is equally well accessible as Cluster 1, despite having a considerably lower *Population Density*. It also differs from Cluster 1 by a significantly lower *Conflict Density* and by holding the highest *EIR* value, paired with high levels of *Immunity* and widespread *Use of bednets*. 

**Cluster 4** scores worst in terms of accessibility, yet the *Conflict Density* is by far the lowest, while the *Population density* is equally low as in Cluster 2. The *EIR* ranges among the lower values, while the *Immunity* is highest and the *Use of bednets* seems to be widely spread. However, the share of *People living on less than 2 US$ per day* is by far the highest here. 



```{r Statistics, message = FALSE}

#Creating a table showing descriptive statistics (mean, sd, min, max) of each cluster

#Loading necessary libraries
library(kableExtra)
library(reshape2)

#Calculating descriptive statistics for each cluster
w<-descr(classD$`1`, stats = c("mean", "sd", "min", "max"), transpose = TRUE)
u<-descr(classD$`2`, stats = c("mean", "sd", "min", "max"), transpose = TRUE)
t<-descr(classD$`3`, stats = c("mean", "sd", "min", "max"), transpose = TRUE)

#Populating the first column of the table with indicator names, to make clear to what variable the values relate 
vars<-c("Child does not sleep under bednet","Conflict density","Dependency ratio","Distance to closest hospital","Distance to closest urban center","Distance to roads","EIR","HIV prevalence","Immunity","People living on less than 2 US$/day","Population change","Secondary or higher education","Stunting children","WOCBA")

#Extracting information of descriptive statistics column-wise from the descr-object
wmeltmean<-melt(w$Mean)
umeltmean<-melt(u$Mean)
tmeltmean<-melt(t$Mean)

wmeltsd<-melt(w$Std.Dev)
umeltsd<-melt(u$Std.Dev)
tmeltsd<-melt(t$Std.Dev)

wmeltmin<-melt(w$Min)
umeltmin<-melt(u$Min)
tmeltmin<-melt(t$Min)

wmeltmax<-melt(w$Max)
umeltmax<-melt(u$Max)
tmeltmax<-melt(t$Max)

#Assembling the single columns to one table
DIANA_summary<-data.frame(
  Indicator = vars,
  Cluster1_mean=wmeltmean,
  Cluster2_mean=umeltmean,
  Cluster3_mean=tmeltmean,
  Cluster1_sd=wmeltsd,
  Cluster2_sd=umeltsd,
  Cluster3_sd=tmeltsd,
  Cluster1_min=wmeltmin,
  Cluster2_min=umeltmin,
  Cluster3_min=tmeltmin,
  Cluster1_max=wmeltmax,
  Cluster2_max=umeltmax,
  Cluster3_max=tmeltmax
  )

#Visualizing the table
kable(DIANA_summary,digits=2,col.names = c("Indicator","Cluster 1","Cluster 2","Cluster 3","Cluster 1","Cluster 2","Cluster 3","Cluster 1","Cluster 2","Cluster 3","Cluster 1","Cluster 2","Cluster 3")) %>% kable_styling(bootstrap_options = c("striped","bordered"), full_width = F)%>%
  add_header_above(c(" " = 1, "Mean" = 3, "Standard deviation" = 3, "Min" = 3, "Max" = 3))%>%
  add_header_above(c(" " = 1, "DIANA" = 12))


#kable_as_image(tab_sumstat, filename = table_sumstat, file_format = "png",
 # latex_header_includes = NULL, keep_pdf = FALSE, density = 300,
 # keep_tex = FALSE)
#library(gridExtra)
#qplot(1:10, 1:10, geom="blank")+theme_bw()+theme(line=element_blank(),text=element_blank())+annotation_custom(grob=tableGrob(tab_sumstat))
```

### Spearman Correlation  
The next section shows correlation matrices for the three clusters derived by each respective clustering method, as well as the correlation matrix for the whole dataset. The **correlations serve as the basis for the Causal Loop Diagrams** which are being generated in the last chapter of this scrpit. Therefore, these plots serve as a tool for visually inspecting the strengths and directions of the relationships together with the information concerning significance and the certainty of the significance. As stated earlier, the used data does not fulfill all the criteria necessary to reliably use Pearsons Correlation Coefficient. Therefore, again the decsion is left between Spearmans rho and Kendalls tau. As Spearmans rho has already successfuly served as distance measure in the clustering process, I will continue working with the Spearmans correlation. 
The plots can be interpreted as follows: The numbers in procent indicated the strength of the relationship. The minus sign together with a number colored in pink represent a negative relationship - this means if one variable value increases, the other one decreases. The absence of an algebraic sign with a number colored in green stands for a positive relationship (if variable a increases, b increases as well). The number of stars indicate the significance level of each correlation:** One star -> p=<01, two stars -> p=<0.5 and three stars -> p=<0.01**. The absece of stars mean no significant correlation.  

```{r Correlation Analysis, message = FALSE}

#Creating a Correlation-plot for each cluster

#Function wich creates customized corrplots
make_corrplot <- function (dat,title)
{
#Loading necessary libraries
library (Hmisc)
library(RColorBrewer)
library(corrplot)
#Generating a correlation matrix using Spearman correlation
cors.test <- rcorr (as.matrix (dat [complete.cases (dat), ],type=c("spearman")))
#Extracting the strength and direction of the correlation from the matrix
cors <- cors.test$r
#Extracting the p-values defining significane from the matrix
ps <- cors.test$P

#Defining color ramp
cramp <- brewer.pal(10, "PiYG")

#Visualize the correlation matrix
corrplot (cors, add = FALSE, method = "shade", shade.col = NA, tl.col = "black",  mar=c(0,0,1,0),cex.main=0.80,tl.cex=0.60,tl.srt=35,
p.mat = ps, sig.level = c (0.01, 0.1, 0.5), insig = "label_sig",pch.cex=1, col = cramp, tl.offset = 1,main=title)
corrplot (cors, bg = "#333333", add = TRUE, type = "lower", method = "number", diag = FALSE, number.cex = 0.5,tl.pos = "n", cl.pos = "n", col = cramp)
#dev.off ()
}

#Executing the function
make_corrplot(DRegion1,"DIANA: Cluster 1 (n=53)")#,"C:/Users/Linda/Documents/CorrMat_Region1.png")
make_corrplot(DRegion2,"DIANA: Cluster 2 (n=19)")#,"C:/Users/Linda/Documents/CorrMat_Region2.png")
make_corrplot(DRegion3,"DIANA: Cluster 3 (n=36)")#,"C:/Users/Linda/Documents/CorrMat_Region3.png")
make_corrplot(GEONS_df,"Whole study region (N=108)")#,"C:/Users/Linda/Documents/CorrMat_All.png")

```



```{r CLD  Pairwise correlation,message = FALSE}

#Calculating pairwise correlations which can be handed over to the function which generates the CLDs later

#Loading necessary libraries
library(Hmisc)

Calculate_Correlation<-function(dat){
  #Calculate correlations as matrix
  Correlations<-rcorr(as.matrix(dat))
  #Extracting p-values and strength and direction of the correlaiton. Also, reshape from matrix to a list containing information on what two variables are involved and what the p-value and the correlation is. 
  p_vals<-reshape2::melt(Correlations$P)[,3]
  corrs<-reshape2::melt(Correlations$r)
  #Writing the p-values to the correlations
  corrs$p<-p_vals
  # Remove correlations of the same variable (e.g. Immunity&Immunity): Only correlations of the same variable will have a perfect correlation of 1. Therefore only values smaller than one are kept.  
  corrs<-subset(corrs,value<1)
  # Keep only significant correlations with p<5%
  corrs<-subset(corrs,p<0.05)
  #Write the result to a new dataframe
  Cor<-data.frame(corrs)
}

#Execute the functions for the three clusters and additionally for the whole dataset as one.
All<-Calculate_Correlation(GEONS_df)
DReg1<-Calculate_Correlation(DRegion1)
DReg2<-Calculate_Correlation(DRegion2)
DReg3<-Calculate_Correlation(DRegion3)

```




### Causal Loop Diagrams  
The package **causalloop** written by Jarrod Dalton was utilized to automatically generate Causal Loop Diagrams from the significant (p<0.5) Spearman correlations. The function needs the input parameters *from*, *to*, *weigth* and *polarity* to generate a CLD. *From* and *to* describes a vulnerability indicator pair, while for the parameter *weight* the strength of the correlation is used. *Polarity* wants to know whether the relationship is of positive or of negative nature. This information is given by whether the correlation value was smaller or greater than 0.  
Red arrows indicate a negative relationship, while green arrows indicate a positive one. The width of the arrows stands for the strength of the relationship (r). 
The differing structures of the CLD's indicate, that the assumption that only one CLD can not sufficiently capture the diverse relationships among the variables throughout the study region, is justified.   


```{r Causal Loop Diagrams Correlation,message = FALSE}

#Creating Causal Loop Diagrams from the previously extracted correlation information

#Loading necessary libraries
library(causalloop)
library(dplyr)


Make_CLD<-function(dat.f,CLD.name){
#Define "from" which variable the relationship starts (first listed variable from the correlation)
from = as.character(dat.f$Var1)
#Define "to" which variable the relationship is directed (second listed variable from the correlation)
to = as.character(dat.f$Var2)
#Define the strength of the relationship (r value from the correlation)
weight = c(abs(dat.f$value))
#Define whether the relationship is of positive or of negative nature (is the r value from the correlation smaller or greater than 0)
Polarity<-c(ifelse(dat.f$value>0,dat.f$polarity<-1,dat.f$polarity<--1))

#Createing a CLD Object
CLD.name<-causalloop:::CLD(from=from,to=to,polarity=Polarity,weight=weight)
CLD.name<-setEdgeFormat(CLD.name,"penwidthAdj",3)

#Plot the CLD
plot(CLD.name, nodes = NULL, steps = 1, recolor = TRUE,
  textWidth = 10)

}

```
### CLD Whole Study Region  
```{r CLD Whole}

Make_CLD(All,CLD_All)


```

### CLD AGNES Cluster 1  
```{r CLD AGN1}

Make_CLD(DReg1,CLD_DReg_1)

```

### CLD AGNES Cluster 2  
```{r CLD AGN2}

Make_CLD(DReg2,CLD_DReg_2)

```

### CLD AGNES Cluster 3  
```{r CLD AGN3}

Make_CLD(DReg3,CLD_DReg_3)

```


```{r refine, Echo=FALSE}
DReg1$edgeIndex<-c(1:38)
DReg1<-DReg1[-c(37,12,10,35,25,29,28,7,27,26),]
#Define "from" which variable the relationship starts (first listed variable from the correlation)
from = as.character(DReg1$Var1)
#Define "to" which variable the relationship is directed (second listed variable from the correlation)
to = as.character(DReg1$Var2)
#Define the strength of the relationship (r value from the correlation)
weight = c(abs(DReg1$value))
#Define whether the relationship is of positive or of negative nature (is the r value from the correlation smaller or greater than 0)
Polarity<-c(ifelse(DReg1$value>0,DReg1$polarity<-1,DReg1$polarity<--1))

#Createing a CLD Object
CLD.Dreg1.ref<-causalloop:::CLD(from=from,to=to,polarity=Polarity,weight=weight)
CLD.Dreg1.ref<-setEdgeFormat(CLD.Dreg1.ref,"penwidthAdj",3)
#CLD.Dreg1.ref$edges$EdgeIndex<-c(1:38)
#CLD.Dreg1.ref$edges
#Plot the CLD
plot(CLD.Dreg1.ref, nodes = NULL, steps = 1, recolor = TRUE,
  textWidth = 10)
```

```{r CLD improvement,eval=FALSE}
#CLD_All$edges

L <- CLD(from=from, to=to,polarity=Polarity,weight=weight)
L$nodes
L$group=c("I","IV","IV","I","I","II","III","II","I","I","III","III")

ndat<-data.frame(
  node=L$nodes[1],
  group=L$group,
  stringsAsFactors = FALSE  )

L <- addNodeData(L, ndat)

L<-addNodeGroup(L,groups=c("I","II","III","IV"))
L<-setNodeFormat(L,groups=c("I","II","III","IV"),style="solid",color=c("DarkOrange","Gold","DodgerBlue","LightSlateGrey"),shape="box")
plot(L)


L<-setEdgeFormat(L,"color","red",polarity=-1)
L<-setEdgeFormat(L,"penwidthAdj",2)
L<-setEdgeFormat(L,"arrowhead","vee")

plot(L, nodes = NULL, steps = 1, recolor = TRUE,
  textWidth = 10)

```


